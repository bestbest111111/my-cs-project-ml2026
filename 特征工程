# -*- coding: utf-8 -*-
import os
import cv2
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

# é…ç½®
TRAIN_IMAGE_ROOT = "C:/Users/19429/Desktop/neu-plant-seedling-classification-2025/dataset-for-task1/dataset-for-task1/train"
TEST_IMAGE_ROOT = "C:/Users/19429/Desktop/neu-plant-seedling-classification-2025/dataset-for-task1/dataset-for-task1/test"
OUTPUT_CSV = "classification_final_high.csv"

def extract_advanced_features(img_path):
    """é«˜çº§ç‰¹å¾æå– - ä¸“é—¨é’ˆå¯¹æ¤ç‰©å¹¼è‹—"""
    try:
        img = cv2.imread(img_path)
        if img is None:
            return None
    except:
        return None
    
    # ä¿æŒé«˜åˆ†è¾¨ç‡
    img = cv2.resize(img, (256, 256))
    
    features = []
    
    # ========== 1. æ¤ç‰©ä¸“ç”¨é¢œè‰²ç‰¹å¾ ==========
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l_channel = lab[:, :, 0].astype(np.float32)
    a_channel = lab[:, :, 1].astype(np.float32)  # ç»¿-çº¢è½´
    b_channel = lab[:, :, 2].astype(np.float32)  # è“-é»„è½´
    
    # å¤šä¸ªç»¿è‰²é˜ˆå€¼æ£€æµ‹
    for threshold in [120, 130, 140]:
        green_mask = a_channel < threshold
        if np.sum(green_mask) > 100:
            green_proportion = np.sum(green_mask) / green_mask.size
            green_l_mean = np.mean(l_channel[green_mask])
            green_a_mean = np.mean(a_channel[green_mask])
            features.extend([green_proportion, green_l_mean, green_a_mean])
        else:
            features.extend([0, 0, 0])
    
    # LABå„é€šé“ç»Ÿè®¡
    for channel in [l_channel, a_channel, b_channel]:
        # åŸºæœ¬ç»Ÿè®¡
        mean_val = np.mean(channel)
        std_val = np.std(channel)
        # å››åˆ†ä½è·
        iqr = np.percentile(channel, 75) - np.percentile(channel, 25)
        # ååº¦
        if std_val > 0:
            skew = np.mean(((channel - mean_val) / std_val) ** 3)
        else:
            skew = 0
        # å³°åº¦
        if std_val > 0:
            kurtosis = np.mean(((channel - mean_val) / std_val) ** 4) - 3
        else:
            kurtosis = 0
        
        features.extend([mean_val, std_val, iqr, skew, kurtosis])
    
    # HSVé¢œè‰²ç›´æ–¹å›¾ï¼ˆæ›´å¤šbinsï¼‰
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    for i in range(3):
        hist = cv2.calcHist([hsv], [i], None, [20], [0, 256])  # 20 bins
        hist = cv2.normalize(hist, hist).flatten()
        features.extend(hist)
    
    # ========== 2. é«˜çº§çº¹ç†ç‰¹å¾ ==========
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
    # å¤šå°ºåº¦æ¢¯åº¦ç‰¹å¾
    gradient_features = []
    for ksize in [3, 5, 7]:
        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=ksize)
        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=ksize)
        gradient_mag = np.sqrt(sobelx**2 + sobely**2)
        
        # æ¢¯åº¦ç»Ÿè®¡
        gradient_features.extend([
            np.mean(gradient_mag),
            np.std(gradient_mag),
            np.percentile(gradient_mag, 90),
            np.percentile(gradient_mag, 95),
            np.percentile(gradient_mag, 99)  # æœ€å¼ºè¾¹ç¼˜
        ])
    
    features.extend(gradient_features)
    
    # Gabor-likeç‰¹å¾ï¼ˆæ¨¡æ‹Ÿï¼‰
    for sigma in [1.0, 2.0]:
        for theta in [0, np.pi/4, np.pi/2, 3*np.pi/4]:
            # ç®€åŒ–Gaboræ»¤æ³¢
            kernel_size = 9
            kernel = cv2.getGaborKernel((kernel_size, kernel_size), sigma, theta, 
                                       10.0, 0.5, 0, ktype=cv2.CV_32F)
            filtered = cv2.filter2D(gray.astype(np.float32), -1, kernel)
            features.extend([np.mean(filtered), np.std(filtered)])
            break  # åªç”¨ä¸€ä¸ªè§’åº¦ä»¥æ§åˆ¶ç‰¹å¾æ•°é‡
        break
    
    # ========== 3. å½¢çŠ¶å’Œè¾¹ç¼˜ç‰¹å¾ ==========
    # è‡ªé€‚åº”é˜ˆå€¼
    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                  cv2.THRESH_BINARY_INV, 11, 2)
    
    # å½¢æ€å­¦æ¸…ç†
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
    
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    shape_features = []
    if contours:
        # å–æœ€å¤§è½®å»“
        main_cnt = max(contours, key=cv2.contourArea)
        area = cv2.contourArea(main_cnt)
        perimeter = cv2.arcLength(main_cnt, True)
        
        if perimeter > 0:
            # ç´§å‡‘æ€§ç‰¹å¾
            circularity = 4 * np.pi * area / (perimeter * perimeter)
            
            # è¾¹ç•Œæ¡†ç‰¹å¾
            x, y, w, h = cv2.boundingRect(main_cnt)
            aspect_ratio = w / (h + 1e-6)
            extent = area / (w * h + 1e-6)
            
            # å‡¸åŒ…ç‰¹å¾
            hull = cv2.convexHull(main_cnt)
            hull_area = cv2.contourArea(hull)
            solidity = area / hull_area if hull_area > 0 else 0
            
            # æœ€å°å¤–æ¥åœ†
            (cx, cy), radius = cv2.minEnclosingCircle(main_cnt)
            circle_area = np.pi * radius * radius
            circle_ratio = area / circle_area if circle_area > 0 else 0
            
            shape_features = [area, perimeter, circularity, aspect_ratio, 
                             extent, solidity, circle_ratio]
        else:
            shape_features = [0] * 7
    else:
        shape_features = [0] * 7
    
    features.extend(shape_features)
    
    # ========== 4. ç»Ÿè®¡ç‰¹å¾ ==========
    # å›¾åƒç†µ
    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
    hist = hist / hist.sum()
    entropy = -np.sum(hist * np.log2(hist + 1e-10))
    features.append(entropy)
    
    # å±€éƒ¨å¯¹æ¯”åº¦
    local_std = cv2.blur(gray.astype(np.float32)**2, (5, 5)) - \
                cv2.blur(gray.astype(np.float32), (5, 5))**2
    local_std = np.sqrt(np.maximum(local_std, 0))
    features.extend([np.mean(local_std), np.std(local_std)])
    
    # è¾¹ç¼˜å¯†åº¦ï¼ˆCannyï¼‰
    edges = cv2.Canny(gray, 30, 100)
    edge_density = np.sum(edges > 0) / edges.size
    features.append(edge_density)
    
    return np.array(features)

def main():
    print("ğŸš€ ç»ˆææ¤ç‰©å¹¼è‹—åˆ†ç±»ç³»ç»Ÿ")
    print("=" * 60)
    
    # 1. æå–ç‰¹å¾
    print("\n[1/4] æå–é«˜çº§ç‰¹å¾...")
    X, y = [], []
    
    categories = sorted([d for d in os.listdir(TRAIN_IMAGE_ROOT) 
                       if os.path.isdir(os.path.join(TRAIN_IMAGE_ROOT, d))])
    
    print(f"å‘ç° {len(categories)} ä¸ªç±»åˆ«")
    
    # æ”¶é›†æ‰€æœ‰ç±»åˆ«æ ·æœ¬
    for cls in categories:
        cls_path = os.path.join(TRAIN_IMAGE_ROOT, cls)
        images = [f for f in os.listdir(cls_path) 
                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        
        print(f"  {cls}: {len(images)} å¼ ")
        
        for img in images:
            feat = extract_advanced_features(os.path.join(cls_path, img))
            if feat is not None:
                X.append(feat)
                y.append(cls)
    
    if not X:
        print("é”™è¯¯: æ²¡æœ‰æå–åˆ°ç‰¹å¾!")
        return
    
    X, y = np.array(X), np.array(y)
    print(f"\nâœ… ç‰¹å¾æå–å®Œæˆ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    
    # 2. ç‰¹å¾é€‰æ‹©ï¼ˆé€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼‰
    print("\n[2/4] ç‰¹å¾é€‰æ‹©...")
    if X.shape[1] > 100:  # å¦‚æœç‰¹å¾å¤ªå¤šï¼Œè¿›è¡Œé€‰æ‹©
        selector = SelectKBest(f_classif, k=min(150, X.shape[1]))
        X_selected = selector.fit_transform(X, y)
        print(f"  ä» {X.shape[1]} ä¸ªç‰¹å¾ä¸­é€‰æ‹© {X_selected.shape[1]} ä¸ªæœ€ä½³ç‰¹å¾")
        X = X_selected
    
    # 3. æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # 4. è®­ç»ƒå¤šä¸ªæ¨¡å‹
    print("\n[3/4] è®­ç»ƒé›†æˆæ¨¡å‹...")
    
    models = {
        'RF': RandomForestClassifier(
            n_estimators=500,
            max_depth=None,
            min_samples_split=2,
            min_samples_leaf=1,
            max_features='sqrt',
            bootstrap=True,
            oob_score=True,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        ),
        'SVM': SVC(
            C=10.0,  # æ›´å¤§çš„Cï¼Œæ›´å°‘æ­£åˆ™åŒ–
            kernel='rbf',
            gamma='scale',
            probability=True,
            random_state=42,
            class_weight='balanced'
        ),
        'GBDT': GradientBoostingClassifier(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=5,
            min_samples_split=10,
            min_samples_leaf=5,
            subsample=0.8,
            random_state=42
        )
    }
    
    # äº¤å‰éªŒè¯è¯„ä¼°
    print("  æ¨¡å‹è¯„ä¼°...")
    best_model_name = None
    best_score = 0
    cv_scores = {}
    
    for name, model in models.items():
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1)
        mean_score = scores.mean()
        cv_scores[name] = mean_score
        
        print(f"  {name}: {mean_score:.4f} (Â±{scores.std()*2:.4f})")
        
        if mean_score > best_score:
            best_score = mean_score
            best_model_name = name
    
    print(f"\n  ğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} ({best_score:.4f})")
    
    # è®­ç»ƒæœ€ä½³æ¨¡å‹
    best_model = models[best_model_name]
    best_model.fit(X_scaled, y)
    
    if hasattr(best_model, 'oob_score_'):
        print(f"  ğŸ“Š è¢‹å¤–å‡†ç¡®ç‡: {best_model.oob_score_:.4f}")
    
    # 5. é¢„æµ‹æµ‹è¯•é›†
    print(f"\n[4/4] é¢„æµ‹æµ‹è¯•é›†...")
    
    test_images = sorted([f for f in os.listdir(TEST_IMAGE_ROOT) 
                         if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
    
    print(f"å‘ç° {len(test_images)} å¼ æµ‹è¯•å›¾ç‰‡")
    
    results = []
    confidence_scores = []
    
    for i, img_name in enumerate(test_images):
        if (i + 1) % 20 == 0:
            print(f"  è¿›åº¦: {i+1}/{len(test_images)}")
        
        feat = extract_advanced_features(os.path.join(TEST_IMAGE_ROOT, img_name))
        
        if feat is not None:
            # å¦‚æœè¿›è¡Œäº†ç‰¹å¾é€‰æ‹©ï¼Œéœ€è¦ç›¸åº”å¤„ç†
            if 'selector' in locals():
                feat = selector.transform([feat])
            
            feat_scaled = scaler.transform(feat)
            
            # ä½¿ç”¨æ‰€æœ‰æ¨¡å‹è¿›è¡Œé¢„æµ‹å¹¶æŠ•ç¥¨
            predictions = []
            confidences = []
            
            for name, model in models.items():
                try:
                    pred = model.predict(feat_scaled)[0]
                    proba = model.predict_proba(feat_scaled)[0]
                    conf = np.max(proba)
                    predictions.append(pred)
                    confidences.append(conf)
                except:
                    continue
            
            if predictions:
                # æŠ•ç¥¨å†³å®šæœ€ç»ˆé¢„æµ‹
                from collections import Counter
                final_pred = Counter(predictions).most_common(1)[0][0]
                avg_confidence = np.mean(confidences) if confidences else 0
            else:
                from collections import Counter
                final_pred = Counter(y).most_common(1)[0][0]
                avg_confidence = 0
        else:
            from collections import Counter
            final_pred = Counter(y).most_common(1)[0][0]
            avg_confidence = 0
        
        results.append([img_name, final_pred, avg_confidence])
        confidence_scores.append(avg_confidence)
    
    # 6. ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœ...")
    df = pd.DataFrame(results, columns=['ID', 'Category', 'Confidence'])
    df[['ID', 'Category']].to_csv(OUTPUT_CSV, index=False)
    
    if os.path.exists(OUTPUT_CSV):
        print(f"âœ… ç»“æœå·²ä¿å­˜: {OUTPUT_CSV}")
        
        # è¯¦ç»†åˆ†æ
        print(f"\nğŸ“Š è¯¦ç»†åˆ†æ:")
        print(f"  æ€»æµ‹è¯•å›¾ç‰‡: {len(df)}")
        
        if confidence_scores:
            print(f"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.4f}")
            print(f"  é«˜ç½®ä¿¡åº¦(â‰¥0.7): {sum(c >= 0.7 for c in confidence_scores)}")
            print(f"  ä½ç½®ä¿¡åº¦(<0.4): {sum(c < 0.4 for c in confidence_scores)}")
        
        print(f"\nğŸ“ˆ é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ:")
        counts = df['Category'].value_counts()
        for category, count in counts.items():
            percentage = count / len(df) * 100
            print(f"  {category}: {count} ({percentage:.1f}%)")
        
        print(f"\nğŸ¯ æ¨¡å‹æ€§èƒ½:")
        for name, score in cv_scores.items():
            print(f"  {name}: {score:.4f}")
        print(f"  æœ€ä½³æ¨¡å‹: {best_model_name} ({best_score:.4f})")
        print(f"  ç‰¹å¾æ•°é‡: {X.shape[1]}")
        
    else:
        print(f"âŒ ä¿å­˜å¤±è´¥!")
        # å°è¯•å…¶ä»–ä½ç½®
        try:
            df[['ID', 'Category']].to_csv("submission_final.csv", index=False)
            print("âœ… å·²ä¿å­˜åˆ° submission_final.csv")
        except:
            print("âŒ å¤‡ä»½ä¿å­˜å¤±è´¥!")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ç¨‹åºå®Œæˆ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
